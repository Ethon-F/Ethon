# Transformer学习——Attention is All You Need 精读

### 前提需要：何为大模型？

**大模型**，通常指参数量巨大、计算能力强的AI模型。通过海量数据和算力训练，具备泛化能力和多任务处理能力。

**特点**：参数量庞大、预训练+微调范式、涌现能力、多模态趋势

#### **典型大模型举例**：

**NLP**——GPT(OpenAI)、BERT(Google)、LLaMA(Meta)、Claude(Anthropic)

**多模态领域**——DALL*E(文本生成图像)、PaLM-E(机器人控制)、Sora(视频生成)

**开源模型**——BLOOM、Falcon、ChatGLM

#### 大模型应用场景

**自然语言处理、计算机视觉、科学研究、企业服务**



## 论文详解

### 前言

```
递归神经网络，特别是长短期记忆[13]和门控递归[7]神经网络，已经被牢固地确立为序列建模和转导问题（如语言建模和机器翻译）的最新方法[35,2,5]。从那以后，许多努力继续推动循环语言模型和编码器-解码架构的边界[38,24,15]。循环模型通常沿输入和输出序列的符号位置进行因子计算。将位置与计算时间中的步骤对齐，它们生成隐藏状态序列ht，作为前一个隐藏状态ht−1的函数和位置t的输入。这种固有的顺序性质排除了训练示例中的并行化，这在较长的序列长度下变得至关重要，因为内存约束限制了跨示例的批处理。最近的工作通过分解技巧[21]和条件计算[32]显著提高了计算效率，同时也提高了模型在后者情况下的性能。然而，顺序计算的基本约束仍然存在。注意机制已经成为各种任务中引人注目的序列建模和转导模型的组成部分，允许对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离[2,19]。然而，在除少数情况外的所有情况下，这种注意机制都与循环网络结合使用。在这项工作中，我们提出了Transformer，这是一种避免重复的模型架构，而是完全依赖于注意机制来绘制输入和输出之间的全局依赖关系。Transformer允许更多的并行化，并且在8个P100 gpu上经过12个小时的训练后，可以达到翻译质量的新状态
```

在语言建模和文本翻译领域，Recurrent neural network（RNN）循环神经网络、LSTM长短期记忆、gated recurrent network 门控递归 是最常用的三种最新方法。循环模型中通常沿输入和输出序列的符号位置进行因子计算。但是**由于RNN它们计算存在着无法改变的固有顺序，是一个线性的计算结构**，所以这种顺序计算模式会成为计算效率的基本约束。为了解决NLP中RNN不能并行计算，从而导致算法效率低的问题，文章提出了Transformer模型。

### 背景

```
减少顺序计算的目标也构成了Extended Neural GPU[16]、ByteNet[18]和ConvS2S[9]的基础，它们都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。在这些模型中，将来自两个任意输入或输出位置的信号关联起来所需的操作数量随着位置之间的距离而增加，ConvS2S为线性，ByteNet为对数。这使得学习远距离位置之间的依赖关系变得更加困难。在Transformer中，这被减少到一个恒定数量的操作，尽管其代价是由于平均注意加权位置而降低了有效分辨率，我们在3.2节中描述了多头注意来抵消这种影响。自我注意，有时被称为内注意，是一种将单个序列的不同位置联系起来的注意机制，以便计算该序列的表示。自我注意已被成功地应用于多种任务中，包括阅读理解、抽象总结、文本蕴涵和学习任务无关的句子表征[4,27,28,22]。端到端记忆网络基于循环注意机制，而不是顺序排列的递归，并且在简单语言问答和语言建模任务中表现良好。然而，据我们所知，Transformer是第一个完全依赖于自关注来计算其输入和输出表示的转导模型，而不使用序列对齐rnn或卷积。在接下来的章节中，我们将描述Transformer，激励自身关注，并讨论它相对于[17,18]和[9]等模型的优势。
```

### 模型架构

![image-20250401204703552](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250401204703552.png)

模型是一个很典型的seq2seq结构（encoder-decoder结构），Encoder里面有N个重复的block结构，Decoder里面也有 N个重复的block结构。Transformer模型的输入为对句子分词后，每个词的one-hot向量组成的一个向量序列，输出为预测的每个词的预测向量。

![v2-f76379ca9b18ad0dfe793316c2271e6f_r](C:\Users\1\Desktop\v2-f76379ca9b18ad0dfe793316c2271e6f_r.png)

 Embedding操作是和翻译模型一起学习的。

#### Encoder结构

Encoder由6个相同的层堆栈而成，即N=6，如图1左侧所示，每一层又由两个子层组成。第一个是多头注意力机制，第二个是简单的全连接的前馈网络

#### Attention Function

```
注意函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。输出以加权和的形式计算
```

#### Scaled Dot-Product Attention【Before Multi-Head Attention】缩放的点乘注意力机制

![image-20250401213319280](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250401213319280.png)

Scaled Dot-Product Attention模块用公式表示为：

![image-20250401213508595](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250401213508595.png)

输入为：

Q（Query，查询）

```
  **当前需要计算注意力的位置/当前需要关注什么/是当前！！！**

**针对输入序列中每个单独词或词组生成的**

  **例如：“我想找一本关于恐龙的书”**
```

K（Key，键）

```
**所有位置的键，用于和Query匹配||表示”内容的关键标签“||是所有！！！！**

**例如：书架上每本书的标签（如“恐龙”“科幻”“历史”），用于匹配查询**
```

V（Value，值）

```
**实际的信息载体||实际的内容**

**例如：书的具体内容，当标签匹配到时，需要获取的内容**
```

以上Q、K的维度均为（L, dk），V的维度为（L，dv），其中dk,dv为特征维度。有个除法是因为防止维度dk太大得到的值就会太大，导致后续的导数会太小但是不知道为什么是根号dk。

最终softmax得到attention权重后，与V相乘，最终Attention输出维度为（L,dv），在Scaled Dot-Product Attention模块中还存在一个可选的Mask模块。

#### Multi-Head Attention（Encoder）多头注意力机制

![image-20250401224517321](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250401224517321.png)

通用公式如下：

<img src="C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250401225036748.png" alt="image-20250401225036748" style="zoom:150%;" />

```
我们发现，与其使用dmodel维的键、值和查询执行单一的注意力函数，不如将查询、键和值分别以不同的、学习过的线性投影h次线性投影到dk、dk和dv维，这是有益的。在查询、键和值的每个投影版本上，我们并行地执行注意力函数，得到d维
```

用了self-Attention机制



单头注意力机制（如基础的Scaled Dot-Product Attention）只能学习一种信息关联模式。

多头注意力将输入序列投影到多个子空间（即“头”），每个头独立学习不同的关注模式，最终合并结果。

投影矩阵是通过模型训练得到的可学习参数，在训练开始时随机初始化，随后通过反向传播优化。

dmodel是多头注意力模块输入与输出张量的通道维度，h为head个数

#### Attention在此模型中的应用

```
在“编码器-解码器注意”层中，查询来自前一个解码器层，而记忆键和值来自编码器的输出。这允许解码器中的每个位置都参与输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意机制，例如
```



##### 1.Transformer中有编码器-解码器注意力子层，这个东西是解码器的一个关键组件，Q来自于上一个解码器，Key和Value来自于编码器的输出，聚焦于编码器的输出信息；

```
Seq2Seq模型中【LSTM+Attention】的瓶颈在于，编码器要将整个输入序列压缩成一个固定长度的vector，导致长序列信息丢失。
Transformer可以通过编码器-解码器注意力，允许解码器直接访问编码器的全部输出，动态选择相关信息。
```



##### 2.每一个encoder层都包含了self-attention自注意力层，在self-attention中，所有的key、Q、Value都来自于同一个地方

```
编码器包含自关注层。在自关注层中，所有的键、值和查询都来自同一个地方，在这种情况下，是编码器中前一层的输出。编码器中的每个位置都可以处理编码器前一层中的所有位置。
```



##### 3.不懂

```
1.自回归生成的核心要求
在生成任务中，解码器需要按顺序逐个生成词，当前词只能依赖已生成的左侧词，不能偷看未来的词。例如，生成第三个词时，只能用第1、2个词的信息，这种就叫自回归，auto-regressive

####Transformer架构存在的问题
解码器的自注意力层理论上可以访问序列的所有位置（包括未来的位置），这与自回归生成冲突。
因此通过掩码，在计算注意力权重时，屏蔽mask掉当前位置右侧的所有未来位置

实现方式：计算注意力分数矩阵、在应用Softmax之前，将未来位置对应的分数设置为极小值
```

#### Position-wise Feed-Forward Network位置前馈网络

进行了Attention操作之后，每一个编码器和解码器中都有全连接的前馈网络，它对每个Postion的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出

![image-20250406212330594](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250406212330594.png)

#### Embeddings & Softmax【有坑没细读】

 

```
what is embedding?
使用某种方式将词向量化，即输入一个词输出该词对应的一个向量。（embedding可以采用训练好的模型如GLOVE等进行处理，也可以直接利用深度学习模型直接学习一个embedding层，Transformer是第二种，自己学一个embedding层）
```



#### Positional Encoding位置嵌入

表示词在序列中的位置

Since without recurrence and convolution，必须注入一些相对或者绝对的位置信息。在Transformer中加入位置编码positional encoding 

![image-20250407133040515](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20250407133040515.png)

其中pos表示位置，i表示dimension。